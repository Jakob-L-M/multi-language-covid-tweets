{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0fc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Imports from plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# sentiment discovery imports\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Lemmatization for wordclouds\n",
    "from textblob import Word\n",
    "\n",
    "# wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Tf-Idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import heapq\n",
    "\n",
    "# utilities\n",
    "from tqdm.auto import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afbef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('pre-processed-data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a17b211",
   "metadata": {},
   "source": [
    "## Initial Language Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = pd.read_csv('TweetsAboutCovid-19.csv')\n",
    "\n",
    "languages = np.array(initial['language'], dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b28c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang, count = np.unique(languages, return_counts=True)\n",
    "\n",
    "order = np.argsort(count)[::-1]\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "sns.barplot(x=lang[order], y=np.log2(count[order])+1)\n",
    "plt.yticks(range(1,20,2), [2**i for i in range(0,19,2)])\n",
    "plt.ylabel('Number of Tweets in Dataset')\n",
    "plt.xlabel('Language Code')\n",
    "plt.title('Initial distribution of languages', fontsize=20);\n",
    "plt.savefig('inital_languages.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a71a68",
   "metadata": {},
   "source": [
    "## Hashtag and Mentions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083580a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = []\n",
    "for tweet in initial['tweet']:\n",
    "    mention = re.findall(r'(?:^|[^a-zA-Z0-9_＠!@#$%&*])(?:(?:@|＠)(?!\\/))([a-zA-Z0-9/_.]{1,15})(?:\\b(?!@|＠)|$)', tweet)\n",
    "    \n",
    "    mentions.append(mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf679b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial['mentions'] = mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.subplots_adjust(hspace= 0.25)\n",
    "# Initiating subplots\n",
    "sub1 = fig.add_subplot(2,2,1) # two rows, two columns, fist cell\n",
    "sub2 = fig.add_subplot(2,2,2) # two rows, two columns, second cell\n",
    "sub3 = fig.add_subplot(2,2,(3,4)) # two rows, two colums, combined third and fourth cell\n",
    "\n",
    "languages = ['nl', 'en', 'de', 'fr', 'in', 'ja', 'pt', 'es']\n",
    "study_hashtags = [0.16, 0.14, 0.25, 0.16, 0.1, 0.04, 0.11, 0.12]\n",
    "study_mentions = [0.62, 0.5, 0.28, 0.55, 0.77, 0.48, 0.45, 0.62]\n",
    "\n",
    "# Hashtags\n",
    "for i in range(len(languages)):\n",
    "    temp = initial[initial['language'] == languages[i]]\n",
    "    \n",
    "    hashtag_ration = sum(temp['hashtags'].str.len() > 2)/len(temp)\n",
    "    if i == 0:\n",
    "        sub1.bar(x=i-0.2, height=hashtag_ration, width=0.4, color='darkblue', label='Dataset Ratio')\n",
    "        sub1.bar(x=i+0.2, height=study_hashtags[i], width=0.4, color='darkred', label='Study Ratio')\n",
    "    else:\n",
    "        sub1.bar(x=i-0.2, height=hashtag_ration, width=0.4, color='darkblue')\n",
    "        sub1.bar(x=i+0.2, height=study_hashtags[i], width=0.4, color='darkred')\n",
    "\n",
    "sub1.set_ylabel('Ratio of Tweets including Hashtags')    \n",
    "sub1.legend()\n",
    "sub1.set_xticks(range(len(languages)), languages);\n",
    "sub1.set_title('Usage of Hashtags')\n",
    "\n",
    "# Mentions\n",
    "for i in range(len(languages)):\n",
    "    temp = initial[initial['language'] == languages[i]]\n",
    "    \n",
    "    mentions_ration = sum(temp['mentions'].str.len() > 0)/len(temp)\n",
    "    if i == 0:\n",
    "        sub2.bar(x=i-0.2, height=mentions_ration, width=0.4, color='darkblue', label='Dataset Ratio')\n",
    "        sub2.bar(x=i+0.2, height=study_mentions[i], width=0.4, color='darkred', label='Study Ratio')\n",
    "    else:\n",
    "        sub2.bar(x=i-0.2, height=mentions_ration, width=0.4, color='darkblue')\n",
    "        sub2.bar(x=i+0.2, height=study_mentions[i], width=0.4, color='darkred')\n",
    "\n",
    "sub2.set_ylabel('Ratio of Tweets including Mentions')    \n",
    "sub2.legend()\n",
    "sub2.set_xticks(range(len(languages)), languages);\n",
    "sub2.set_title('Usage of Mentions')\n",
    "\n",
    "\n",
    "# Scatter of change in Hashtag and Mention ratio\n",
    "for i in range(len(languages)):\n",
    "    temp = initial[initial['language'] == languages[i]]\n",
    "    \n",
    "    mentions_ration = sum(temp['mentions'].str.len() > 0)/len(temp)\n",
    "    hashtag_ration = sum(temp['hashtags'].str.len() > 2)/len(temp)\n",
    "    \n",
    "    if i == 0:\n",
    "        sub3.scatter(x=[mentions_ration], y=[hashtag_ration], color='darkblue', label='Dataset Ratio', marker='x')\n",
    "        sub3.scatter(x=[study_mentions[i]], y=[study_hashtags[i]], color='darkred', label='Study Ratio', marker='x')\n",
    "    else:\n",
    "        sub3.scatter(x=[mentions_ration], y=[hashtag_ration], color='darkblue', marker='x')\n",
    "        sub3.scatter(x=[study_mentions[i]], y=[study_hashtags[i]], color='darkred', marker='x')\n",
    "        \n",
    "    sub3.text(x=mentions_ration, y=hashtag_ration+0.02, s=languages[i])\n",
    "    sub3.plot([mentions_ration, study_mentions[i]], [hashtag_ration, study_hashtags[i]], color='black', ls='--')\n",
    "\n",
    "sub3.set_title('Change in Ratios compared to study')\n",
    "sub3.set_ylabel('Ratio of Hashtags')\n",
    "sub3.set_xlabel('Ratio of Mentions')\n",
    "sub3.legend()\n",
    "\n",
    "fig.suptitle('Comparison of Hashtag and Mention usage to Weerkamp et al (2011)', fontsize=16)\n",
    "plt.savefig('hashtag-mentions.pdf', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ee5ed",
   "metadata": {},
   "source": [
    "## Activity and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce8a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perfrom sentiment analysis using textblob\n",
    "# Does ~4k iterations per second\n",
    "polarity = []\n",
    "sentiment = []\n",
    "for tweet in tqdm(df['translation']):\n",
    "    blob = TextBlob(tweet)\n",
    "    pol = 0\n",
    "    for sentence in blob.sentences:\n",
    "        pol += sentence.sentiment.polarity\n",
    "    polarity.append(pol)\n",
    "    sentiment.append('positive' if pol > 0.5 else 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee24590",
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_times = [i.timestamp() for i in df['created_at']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to dataframe\n",
    "df['polarity'] = polarity\n",
    "df['sentiment'] = sentiment\n",
    "df['timestamps'] = posting_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ed4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.violinplot(data=df, y='language', x='timestamps', orient='h', inner=None,\n",
    "               ax=ax, hue='sentiment', split=True, palette={\"positive\": \"r\", \"negative\": \"b\"})\n",
    "\n",
    "# setting axis ticks\n",
    "plt.xticks([1618963200 + i*86400 for i in range(4)], ['21-04-21','21-04-22','21-04-23','21-04-24'])\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(86400/4))\n",
    "ax.xaxis.set_minor_formatter(lambda x, i: str(int((x%86400)//3600)) + ':00')\n",
    "ax.tick_params(which='minor', labelsize=7)\n",
    "ax.tick_params(which='major', length=12)\n",
    "\n",
    "# setting labels\n",
    "plt.ylabel('Language')\n",
    "plt.xlabel('Time (UTC)')\n",
    "plt.title('Activity and sentiment in differnet languages');\n",
    "\n",
    "plt.savefig('activity-sentiment.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097acab",
   "metadata": {},
   "source": [
    "## WordCloud creation using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords from nltk corpus\n",
    "s_words = set(stopwords.words())\n",
    "\n",
    "s_words.add('wa')\n",
    "s_words.add('is')\n",
    "s_words.add('time')\n",
    "s_words.add('year')\n",
    "s_words.add('today')\n",
    "s_words.add('day')\n",
    "s_words.add('amp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists\n",
    "es = []\n",
    "en = []\n",
    "de = []\n",
    "fr = []\n",
    "\n",
    "for entry in tqdm(df.iterrows(), total=len(df), desc='Lemmatizing words and building language documents'):\n",
    "        data = entry[1]\n",
    "\n",
    "        text_wo_stopwords = ' '.join([i for i in data['translation'].split() if Word(i).lemmatize().lower() not in s_words and len(i) > 1])\n",
    "        \n",
    "        if data['language'] == 'es':\n",
    "            es.append(text_wo_stopwords)\n",
    "        if data['language'] == 'en':\n",
    "            en.append(text_wo_stopwords)\n",
    "        if data['language'] == 'de':\n",
    "            de.append(text_wo_stopwords)\n",
    "        if data['language'] == 'fr':\n",
    "            fr.append(text_wo_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d318b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.array([' '.join(es), ' '.join(en), ' '.join(de), ' '.join(fr)])\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(mat)\n",
    "X_words = np.array(vectorizer.get_feature_names())\n",
    "X, X_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0a75bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_arr = []\n",
    "k = 25\n",
    "\n",
    "# iterate over languages and extract the 25 most relavent words\n",
    "for lang in tqdm(range(4), desc='building output'):\n",
    "\n",
    "    arr = np.array(X[lang].todense())[0]\n",
    "\n",
    "    # remove all zero entries to speed up top k selection\n",
    "    words = X_words[arr != 0]\n",
    "    arr = arr[arr != 0]\n",
    "    top_k_ind = heapq.nlargest(k, enumerate(arr), key=lambda x: x[1])\n",
    "\n",
    "    # separate the wrights and normalize them\n",
    "    top_k_w = [i[1] for i in top_k_ind]\n",
    "    top_k_w = list(np.array(top_k_w)/sum(top_k_w))\n",
    "\n",
    "    # add all data to the output array\n",
    "    temp = {'language': lang, 'words': [], 'weights': []}\n",
    "    for i in range(0, len(top_k_ind)):\n",
    "        # transform to uppercase for uniform appearance\n",
    "        temp['words'].append(words[top_k_ind[i][0]].upper())\n",
    "\n",
    "        # round to 5 digits to save space in export\n",
    "        temp['weights'].append(float(f'{top_k_w[i]:.5f}'))\n",
    "\n",
    "    language_arr.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f9c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2,2, figsize=(12,8))\n",
    "languages = ['Spanish', 'English', 'German', 'French']\n",
    "for j in range(4):\n",
    "    wc = WordCloud(background_color=\"white\", height=400, width=600)\n",
    "    wc.fit_words({language_arr[j]['words'][i]: language_arr[j]['weights'][i] for i in range(k)})\n",
    "\n",
    "    ax[j//2, j%2].imshow(wc)\n",
    "    ax[j//2, j%2].axis('off')\n",
    "    ax[j//2, j%2].set_title(languages[j], fontsize=15)\n",
    "    \n",
    "plt.suptitle('Most relevant words in different languages', fontsize=20)\n",
    "plt.savefig('wordclouds.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic untilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# regex cleaning\n",
    "import re\n",
    "\n",
    "# progress bar\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42392fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset avalible at:\n",
    "# https://www.kaggle.com/datasets/komalkhetlani/tweets-about-covid19-all-over-the-world\n",
    "df = pd.read_csv('TweetsAboutCovid-19.csv', low_memory=False, index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns not usful for the analysis.\n",
    "\n",
    "# We will be looking at text content so the 'thumbnail' and 'video' columns will be removed\n",
    "\n",
    "# Further we remove the point column since less than 1% of the entries carry a value.\n",
    "\n",
    "# 'timezone' is always 0. There for the column does not carry information and will be removed.\n",
    "\n",
    "# Like 'timezone', 'retweet' is always False and therefore unneccessary\n",
    "\n",
    "# Lastly 'cashtags' only carries values for 1'424 Tweets. My analysis will not revolve around finances\n",
    "# so this columns will also be removed\n",
    "\n",
    "df = df.drop(columns=['video', 'thumbnail', 'place', 'timezone', 'cashtags', 'retweet'])\n",
    "\n",
    "# We will also drop all columns with NA values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb6e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns created_at and date combined with time are also redundant.\n",
    "# We will transform the created_at column to datetime objects and remove date and time\n",
    "df = df.drop(columns=['date', 'time'])\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dae83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the current DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb2c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next up we will transform the count columns replies, retweets and likes to integers instead of floats\n",
    "df['likes_count'] = df['likes_count'].astype(np.int32)\n",
    "df['replies_count'] = df['replies_count'].astype(np.int32)\n",
    "df['retweets_count'] = df['retweets_count'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next is the language selection.\n",
    "\n",
    "# We will first look at the distribuions\n",
    "language, counts = np.unique(df['language'], return_counts=True)\n",
    "\n",
    "# Order the top 11 decreasing. These are all languages with more than 10.000 Tweets.\n",
    "order = np.argsort(counts)[:-11:-1]\n",
    "\n",
    "# print the language codes together with their counts\n",
    "\n",
    "# en: English\n",
    "# es: Spanish\n",
    "# in: Indonesian\n",
    "# pt: Portugise\n",
    "# hi: Hindi\n",
    "# fr: French\n",
    "# de: German\n",
    "# und: undecicive (will ignore)\n",
    "# ja: Japanise\n",
    "# tr: Turkish\n",
    "print(language[order])\n",
    "print(counts[order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e8b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the array, so that only tweets of significantly big languages are present.\n",
    "# This removes around 10% of data (80.043 Tweets).\n",
    "df = df[df['language'].isin(['en', 'es', 'fr', 'de'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc9d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next up we will remove URL and replace @mentions with a generic @user.\n",
    "# To accomplish this regex expressions are used.\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # convert to lowercase\n",
    "    text = ' ' + text.lower() + ' '\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '', text)\n",
    "    \n",
    "    # replace @ mentions and hashtags\n",
    "    text = re.sub(r' [@#]\\w+', '', text)\n",
    "    \n",
    "    # remove digits\n",
    "    text = re.sub(r' \\d+', '', text)\n",
    "    \n",
    "    # remove special characters\n",
    "    text = re.sub(r'[\\-\\:]', '', text)\n",
    "    \n",
    "    # remove emojis\n",
    "    text = re.sub(r'[\\U0001F300-\\U0001F5FF|\\U0001F1E6-\\U0001F1FF|\\U00002700-\\U000027BF|\\U0001F900-\\U0001F9FF|\\U0001F600-\\U0001F64F|\\U0001F680-\\U0001F6FF|\\U00002600-\\U000026FF]', '', text)\n",
    "    \n",
    "    # remove whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "clean_text('https://some-website.com/tweet-test.html ThIs is a TEST for my project at @aalto University 2022! Woho üòäüëç')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e8f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the cleaning funciton to all tweets. Takes ~30sec\n",
    "cleaned_tweets = []\n",
    "content_left = []\n",
    "\n",
    "for row in tqdm(df.copy().iterrows(), total=len(df), desc='Cleaning tweets'):\n",
    "    data = row[1]\n",
    "\n",
    "    cleaned = clean_text(data['tweet'])\n",
    "    \n",
    "    # create a filter to discard empty tweets after cleaning\n",
    "    if cleaned == '':\n",
    "        content_left.append(False)\n",
    "    else:\n",
    "        content_left.append(True)\n",
    "    \n",
    "    cleaned_tweets.append(cleaned)\n",
    "\n",
    "df['cleaned_tweets'] = cleaned_tweets\n",
    "\n",
    "print('Number of empty tweets after cleaning:', len(df) - sum(content_left))\n",
    "\n",
    "# remove empty tweets\n",
    "df = df[content_left]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ddbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translating non-english tweets\n",
    "sum_of_characters = 0\n",
    "for t in df[df['language'] != 'en']['cleaned_tweets']:\n",
    "    sum_of_characters += len(t)\n",
    "print('Total number of characters in all non-english tweets:', sum_of_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dfd0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now remove non english\n",
    "df = df[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49947bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9602dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving pre-processed data set\n",
    "df.to_pickle('pre-processed-data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
